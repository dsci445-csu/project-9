---
title: "Actionable League of Legends Game Outcome Prediction Using Machine Learning Methods"
author: "By Nick Li, Ryan Stevens, Elle Angell, Coleman Garnier"
output:
  ioslides_presentation:
    widescreen: true
    incremental: false
---

## Motivation:
League is the World's Largest E-sport and it's not close.

* 2024 World Finals: 50 million concurrent peak viewership.
* Most active E-sport at CSU
* Many players want to get better

## Research Questions:
1. Which data best accounts for the outcome of a specific match?
2. Is it possible to provide meaningful feedback to a player on a game using a model?

## Data Overview:
* First party source: Riot Games Developer API, patch 25.22 (Nov 1-3, 2025)
* Collected matches and timelines data through Python
* Highly nested
* Focus: average match MMR of 'Diamond II' (top 1.5% skill)

## Our Data:
* 'Small' Dataset n=250 matches (2,500 players)
* 'Medium' Dataset n=5,000 matches (50,000 players)
* 'Large' Dataset n=50,000 (n=500,000 players)

## Timelines vs Matches:
* Timelines data is mostly superior
* Matches data had better performance on preliminary testing
* In the future...

## Data Features:
* Binary classification problem (win/loss)
* Over 120 potential predictor columns in raw data
* Our dataset used 20

## Turrets:
* Initially accounted for roughly 89% of variance
* Highly colinear with outcome 

## The Research:
Nonlinear Models Dominate:

* Gonzalez, 2023, Deep Neural Network: 89.8%
* Zamir et al., 2024, RNN: 86.7%
* Silva et al., 2023, RNN: 73% (at 15m)
* Chen et al., 2021, Log. Regression: 60.24%

## Fitting a GLM
* Using logistic regression to predict game outcome against all of our player performance metrics (minus turrets)
* 70/30 split between our train/test sets
* Built our GLM on the training set and applied to test data
* Result: ~75.1% accuracy- highest of the all the models we built

```{r, echo = FALSE, warning=FALSE, message=FALSE}
library(parsnip)
library(tidymodels)
library(fastshap)
df <- read.csv("data/d2_250_performance.csv")
df$win <- as.integer(df$win == "True")
df <- df[df$lane != "NONE", ]
df <- subset(df, select=-turretsLost)
df <- subset(df, select=-turretTakedowns)
df$position = NULL
df$win <- factor(df$win)
X <- model.matrix(win ~ . - 1, data=df)
X <- as.data.frame(X)
y <- df$win

set.seed(67)
nrow <- nrow(df)
index <- sample(1:nrow, size = nrow * 0.7)
X_train <- X[index, ]
y_train <- y[index]
X_test <- X[-index, ]
y_test <- y[-index]

glm <- 
  logistic_reg() %>%
  set_engine("glm", family=binomial()) %>%
  fit_xy(x = X_train, y = y_train)
glm_pred <- predict(glm, new_data = X_test, type="prob")[[".pred_True"]]
glm_pred_class <- ifelse(glm_pred > 0.5, "True", "False")
glm_pred_class <- factor(glm_pred_class, levels = levels(y_test))
print(paste("GLM Test Accuracy: ",mean(glm_pred_class == y_test)))

```


## Fitting an SVM
* Goal: Predict game outcome (W/L) based on player performance metrics
* Metrics: kills, deaths, dragonKills, baronKills, firstBloodKill, firstBloodAssist
* Performance: Accuracy = 0.72, Misclassification Rate: 27.64%
* Confusion Matrix: Accurately vs. Falsely predicted

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# fitting SVM w/ accuracy report, misclassication rate, confusion matrix
library(e1071)
library(dplyr)
library(tidyr)
library(tibble)
library(tidymodels)
library(ggplot2)
library(parsnip)
library(fastshap)
library(kernlab)
library(viridis)

df <- read.csv("./data/d2_250_performance.csv")
# predictors
# Source: https://stackoverflow.com/questions/33930188/convert-dataframe-column-to-1-or-0-for-true-false-values-and-assign-to-dataf
X <- df %>%
select(deaths, dragonKills, baronKills, firstBloodKill, firstBloodAssist, kills) %>% 
mutate(firstBloodKill = as.integer(as.logical(firstBloodKill)),
       firstBloodAssist = as.integer(as.logical(firstBloodAssist)))
# target = outcome of game
y <- as.factor(df$win)
# split into training and test sets (70 train 30 test)
set.seed(67)
nrow <- nrow(df)
split_data <- sample(1:nrow, size = nrow * 0.7)
X_train <- X[split_data, ]
y_train <- y[split_data]
X_test <- X[-split_data, ]
y_test <- y[-split_data]
# scaling X
# Source: https://stackoverflow.com/questions/57421329/how-to-scale-test-data-with-respect-of-train-data
X_train_scaled = scale(X_train)
X_test_scaled = scale(X_test,
                      center = attr(X_train_scaled, "scaled:center"),
                      scale = attr(X_train_scaled, "scaled:scale"))
# convert to df to train on svm
df_train <- data.frame(y = factor(y_train), X_train_scaled)
df_test <- data.frame(X_test_scaled)
# train svm
svm_model <- svm(y ~ ., data = df_train, kernel = "radial")
#plot(svm_model)
y_pred <- predict(svm_model, newdata = df_test)

# accuracy 
eval_df <- tibble(
  win = factor(y_test),
  .pred_class = factor(y_pred)
)
svm_accuracy <- accuracy(eval_df, truth = win, estimate = .pred_class)$.estimate
# confusion matrix to check accuracy
conf_matrix <- table(predicted = y_pred, actual = y_test)
# misclassifcation rate: % of wrong predictions
misclass_rate <- mean(y_pred != y_test) * 100
```

```{r, fig.width=6, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE}
# graph
df$firstBloodKill <- as.integer(as.logical(df$firstBloodKill))
df$firstBloodAssist <- as.integer(as.logical(df$firstBloodAssist))
df$lane <- as.factor(df$lane)
df <- df[df$lane != "NONE", ]
df <- subset(df, select =- turretsLost)
df <- subset(df, select =- turretTakedowns)
df$position <- NULL
df$win <- factor(df$win)

# split into training and test sets (70 train 30 test)
set.seed(67)
nr <- nrow(df)
split_data <- sample(seq_len(nr), size = nrow * 0.7)
train <- df[split_data, ]
test <- df[-split_data, ]

# converts df to numeric matrix for SVM
X_train_numeric <- model.matrix(win ~ . -1, data = train) |>
                                as.data.frame()
# remove columns with 0 variance (0 variance was causing errors)
X_train_numeric <- X_train_numeric |> select(where(~ var(.) != 0))
# stores target variable (win) for training (True or False)
y_train <- train$win
# train svm with ksvm (kernel package) for shap+graph
# converts df to numeric matrix with y_train as the target
svm_model_temp <- (ksvm(as.matrix(X_train_numeric), y_train, kernel = "rbfdot", C = 1, kpar = "automatic", prob.model = TRUE))
# get predictions from model for shap
pred_wrapper <- function(object, newdata) {
  predict(object, newdata = as.matrix(newdata), type = "probabilities")[, "True"]
}
set.seed(123)
# extracts importance info for graph
imp <- fastshap::explain(object = svm_model_temp, X = X_train_numeric, y = y_train, pred_wrapper = pred_wrapper, nsim = 20) |>
                         as.data.frame() |>
                         summarise(across(everything(), ~ mean(abs(.)))) |> # mean absolute SHAP value
                         tidyr::pivot_longer(cols = everything(), names_to = "variable", values_to = "importance") |>
                         arrange(desc(importance))

ggplot(imp, aes(x = reorder(variable, importance), y = importance)) +
  geom_col(fill ="blue") +
  coord_flip() +
  labs(
    title = "SVM Predictor Importance",
    x = "Predictor",
    y = "Importance"
  ) +
  theme_minimal()
```


## Fitting a Random Forest

* Used in research for similar games
* Performs its own feature selection
* GINI Impurity used
* 0.78% accuracy

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(parsnip)
library(tidymodels)
library(fastshap)

df <- read.csv("./data/d2_500_10_performance.csv")
df <- df[df$lane != "NONE", ]
df <- subset(df, select=-turretsLost)
df <- subset(df, select=-turretTakedowns)
df$position = NULL
df$win <- factor(df$win)

nr <- nrow(df)
index <- sample(1:nr, size = nr * 0.9)
train <- df[index, ]
test <- df[-index, ]

nr <- nrow(train)
index <- sample(1:nr, size = nr * 0.1)
s_train <- train[index, ]

rf_spec <-
  rand_forest(
    mtry = tune(),
    min_n = tune(),
    trees = 500
  ) |>
  set_engine("ranger", importance="impurity") |>
  set_mode("classification")

rf_wf <-
  workflow() |>
  add_model(rf_spec) |>
  add_formula(win ~ .)

folds <- vfold_cv(s_train, v=5, strata=win)

rf_grid <- grid_regular(
  mtry(range = c(2, 10)),
  min_n(range = c(2, 20)),
  levels = 6
)

rf_tuned <- tune_grid(
  rf_wf,
  resamples=folds,
  grid = rf_grid,
  metrics=metric_set(accuracy)
)

rf_results <- collect_metrics(rf_tuned)

best_params <- select_best(rf_tuned, metric="accuracy")
final_rf <- finalize_workflow(rf_wf, best_params)

rf <- fit(final_rf, train)
```

```{r}

pred <- predict(rf, new_data=test, type="class")

eval_df <- tibble(
  win = test$win,
  .pred_class = pred$.pred_class
)

accuracy(eval_df, truth = win, estimate = .pred_class)$.estimate
table(eval_df$win, eval_df$.pred_class)
```

## Cross Validation on RF

```{r, echo=FALSE, message=FALSE, warning=FALSE}
acc_results <- rf_results |> dplyr::filter(.metric == "accuracy")

ggplot(acc_results, aes(x = mtry, y = min_n, fill = mean)) +
  geom_tile() +
  labs(
    title = "5-Fold CV Accuracy for Random Forest",
    x = "mtry",
    y = "min_n",
    fill = "Accuracy"
  )
```

## RF Influential Predictors

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tibble)
library(dplyr)
library(ggplot2)

imp <- rf |> extract_fit_engine() |> ranger::importance() |>
  enframe(name="variable", value="importance") |>
  arrange(desc(importance))

ggplot(imp, aes(x = reorder(variable, importance), y = importance)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Random Forest Predictor Importance",
    x = "Predictor",
    y = "Importance"
  ) 

```

## What is SHAP?
* SHapley Additive exPlanations
* Model-agnostic
* Considers all possible feature combinations using Monte Carlo sampling
* Explains feature contribution to a desicion

## SHAP on Random Forest
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(fastshap)

X <- s_train %>% select(-win)

predict(rf, new_data = head(X), type = "prob") %>% names()

f_fun <- function(object, newdata) {
  newdata <- as.data.frame(newdata)
  predict(object, new_data = newdata, type = "prob") %>%
    dplyr::pull(".pred_True") 
}

set.seed(1)
shap_vals <- fastshap::explain(
  rf,
  X = X,
  pred_wrapper = f_fun,
  nsim = 5
)
```

```{r}
print("Mean impact of deaths on game outcome:")
mean(abs(shap_vals[ ,1]))

```

## SHAP on Random Forest

```{r}
shap_long <- as.data.frame(shap_vals) %>%
  mutate(row_id = row_number()) %>%
  pivot_longer(
    cols = -row_id,
    names_to = "feature",
    values_to = "shap"
  )

feature_order <- shap_long %>%
  group_by(feature) %>%
  summarise(mean_abs_shap = mean(abs(shap), na.rm = TRUE)) %>%
  arrange(mean_abs_shap) %>%
  pull(feature)

shap_long$feature <- factor(shap_long$feature, levels = feature_order)

ggplot(shap_long, aes(x = shap, y = feature)) +
  geom_point(alpha = 0.25) +
  labs(
    title = "SHAP Summary Plot",
    x = "SHAP Value",
    y = "Feature"
  )
```

## Interpreting Results

Example SHAP output 
We can input this line:
fastshap::explain(rf, X=X_test, pred_wrapper=predict_fn, newdata=examplegame, nsim=1000)

And get something like this:
"deaths 1.755, baronkills -6.695, firstBloodKillFalse -8.228, ..., goldEarned -2.382e-02, clearedPings 0.000"

## SHAP Real Example
*feed the model a game one of us played*

## Results

- GLM achieved **75.1%** accuracy  
- Random Forest: **74.7%** accuracy
- SVM: **72.0%** accuracy  
- Consistent top predictors: **deaths**, **goldEarned**  
- Lowest predictors: **first blood–related stats**

## Discussion

- **GoldEarned** → leads to item power → greater access to victory conditions  
- **Deaths** → enemy gold + **loss of tempo** (6–56 seconds downtime)  
- First blood metrics → minimal long-term predictive value  


## Conclusion

- Pipeline integrates **ML models + SHAP** for interpretable insights  
- High predictive performance across all three models  
- Influential factors: **gold**, **kills**, **deaths**, **vision control**  
- First blood → not influential  

- Supports esports analytics, automated coaching, personalized tools

## Future Work
* Temporal version
* Making an actual application

## References

https://x.com/lolesports/status/1859295721956024521
