---
title: "ryan-eda"
output: html_document
---

## 1.1: Interaction Testing
The `d2_250_pwdw.csv` file is imported from the python EDA. Doing interactions in python is painful so here I am testing whether the interaction between wards and position is meaningful. I want to initially do this with a linear model and move out from there.

```{r}
df <- read.csv("data/d2_250_pwdw.csv")
df$win <- df$win == "True"
```

```{r}
set.seed(67)
nrow <- nrow(df)
index <- sample(1:nrow, size = nrow * 0.7)
train <- df[index, ]
test <- df[-index, ]
```

```{r}
fit_basic <- lm(win ~ wards + position  + duration, data=train)
fit_interact <- lm(win ~ wards * position + duration, data=train)

pred_basic <- predict(fit_basic, newdata=test)
pred_interact <- predict(fit_interact, newdata=test)

mse_basic <- mean((test$win - pred_basic) ^ 2)
mse_interact <- mean((test$win - pred_interact) ^ 2)

print(c(mse_basic, mse_interact, var(test$win)))
```

It's safe to conclude that linear interactions between ward and positions do not provide a meaningful difference.

## 1.2: Big LM
Time to add more predictors

```{r}
df <- read.csv("data/d2_250_big.csv")
df$win <- df$win == "True"
```

```{r}
set.seed(67)
nrow <- nrow(df)
index <- sample(1:nrow, size = nrow * 0.7)
train <- df[index, ]
test <- df[-index, ]
```

```{r}
fit_big <- lm(win ~ ., data=train)
pred_big <- predict(fit_big, newdata=test)
mse_big <- mean((test$win - pred_big)^2)
mse_big
summary(fit_big)
```

```{r}
fit_reduced <- lm(win ~ ., data = train)
pred_reduced <- predict(fit_reduced, newdata=test)
mse_reduced <- mean((test$win - pred_reduced)^2)
mse_reduced
var(test$win)
```

My conclusion here is that pure playstyle predictors (pings, wards) are overall poor predictors of win. Performance-based predictors like turret takedowns or cctime are good predictors but are obviously correlated with the win through confounding factors like the gold they provide and the fact that already winning teams are going to have an easier time taking turrets.

I'm not sure exactly how to use this model to analyze a specific game yet either.

As a result, I think that I want to try these things in order:
1) Using the model to provide feedback on a specific game.
2) Fitting non-linear models to see if they can make sense of things better
3) Trying to focus on performance-based metrics to provide feedback (IE: "your CS is bad" or "you die too much")


## 1.3: Performance LM

```{r}
df <- read.csv("data/d2_250_performance.csv")
df$win <- df$win == "True"
#df <- df[df$lane != "NONE"]
```

```{r}
set.seed(67)
nrow <- nrow(df)
index <- sample(1:nrow, size = nrow * 0.7)
train <- df[index, ]
test <- df[-index, ]
```

```{r}
colnames(df)
```

```{r}
fit_big <- lm(win ~ goldEarned * lane, data=train)
pred_big <- predict(fit_big, newdata=test)
mse_big <- mean((test$win - pred_big)^2)
mse_big
summary(fit_big)
```

## 1.4: RF
Let's fit a random forest model

```{r}
library(tidymodels)
library(tibble)
```

```{r}
df <- read.csv("data/d2_250_performance.csv")
df <- df[df$lane != "NONE", ]
df$position = NULL
df$win <- factor(df$win)
X <- model.matrix(win ~ . - 1, data=df)
X <- as.data.frame(X)
y <- df$win
```

```{r}
set.seed(67)
nrow <- nrow(df)
index <- sample(1:nrow, size = nrow * 0.7)
X_train <- X[index, ]
y_train <- y[index]
X_test <- X[-index, ]
y_test <- y[-index]
```

```{r}
rf <- rand_forest(
  mtry= 5,
  trees = 500,
  min_n = 5,
) %>%
  set_mode("classification") %>%
  set_engine("ranger", importance = "impurity") %>%
  fit_xy(X_train, y_train)
```

```{r}
pred <- predict(rf, new_data=X_test, type="class")

eval_df <- tibble(
  win = y_test,
  .pred_class = pred$.pred_class
)

accuracy(eval_df, truth = win, estimate = .pred_class)$.estimate
```
## 1.5: SHAP
Let's use SHAP to evaluate gameplay based on our random forest model.

```{r}
library(fastshap)
```

Okay let's take a random match:
```{r}
example_index <- 20
example <- X_test[example_index, ]
print(y_test[example_index])
print(paste("Deaths: ", example$deaths))
```
Looks like this game was a loss. The player also had 7 deaths (which is a lot and deaths are obviously not good!) So perhaps this contributed to the outcome.

```{r}
predict_fn <- function(object, newdata){
    predict(object, new_data = newdata, type = "prob")$.pred_True
}
```

```{r}
set.seed(67)

testexplain <- fastshap::explain(rf, X=X_test, pred_wrapper=predict_fn, newdata=example, nsim=1000,adjust=TRUE)
testexplain
```
This is a bunch of explanation of what contributed to this particular game. So we can see that deaths reduced the predicted win chance by 0.02 (the adjust setting makes this an actual probability). Turrets lost, however, accounted for an overwhelming majority of the problem.

Next, since the turretslost stat is so closely tied to winning/losing and perhaps doesnt even account well for individual performance, I want to fit a random forest model that does not include this predictor.

## 1.6:
RF without turrets lost.
```{r}
library(fastshap)
library(tidymodels)
library(tibble)
```

```{r}
df <- read.csv("data/d2_250_performance.csv")
df <- df[df$lane != "NONE", ]
df <- subset(df, select=-turretsLost)
df <- subset(df, select=-turretTakedowns)
df$position = NULL
df$win <- factor(df$win)
X <- model.matrix(win ~ . - 1, data=df)
X <- as.data.frame(X)
y <- df$win

set.seed(67)
nrow <- nrow(df)
index <- sample(1:nrow, size = nrow * 0.7)
X_train <- X[index, ]
y_train <- y[index]
X_test <- X[-index, ]
y_test <- y[-index]
```

```{r}
rf <- rand_forest(
  mtry= 5,
  trees = 500,
  min_n = 5,
) %>%
  set_mode("classification") %>%
  set_engine("ranger", importance = "impurity") %>%
  fit_xy(X_train, y_train)

pred <- predict(rf, new_data=X_test, type="class")

eval_df <- tibble(
  win = y_test,
  .pred_class = pred$.pred_class
)

accuracy(eval_df, truth = win, estimate = .pred_class)$.estimate
```

```{r}
predict_fn <- function(object, newdata){
    predict(object, new_data = newdata, type = "prob")$.pred_True
}
```

```{r}
example_index <- 50
example <- X_test[example_index, ]

predict(rf, new_data=example, type="prob")$.pred_True
```
```{r}
example
```

```{r}
set.seed(67)

testexplain <- fastshap::explain(rf, X=X_test, pred_wrapper=predict_fn, newdata=example, nsim=1000,adjust=TRUE)
testexplain[1, ]
```

## 1.7:
Let's fit a `glm` model using logistic regression.

## 1.8:
Fit a `rf` model using created statistics. (e.g. $UtilityScore=wardsPlaced+totalPings*0.1$)

## 1.9:
Pick the best SHAP model and make a dataframe containing the SHAP explanations for the dataset. 
We can then use this to figure out variance and what variables are often explaining the choices.

## Remaining Plan:
Find a way to extract the top 3 positive and top 3 negative contributors from a given match, then map these to feedback we can give the player. (e.g. Deaths was -0.18, so we can give advice that they die too much)


