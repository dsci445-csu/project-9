---
title: "ryan-eda"
output: html_document
---

## 1.1: Interaction Testing
The `d2_250_pwdw.csv` file is imported from the python EDA. Doing interactions in python is painful so here I am testing whether the interaction between wards and position is meaningful. I want to initially do this with a linear model and move out from there.

```{r}
df <- read.csv("data/d2_250_pwdw.csv")
df$win <- df$win == "True"
```

```{r}
set.seed(67)
nrow <- nrow(df)
index <- sample(1:nrow, size = nrow * 0.7)
train <- df[index, ]
test <- df[-index, ]
```

```{r}
fit_basic <- lm(win ~ wards + position  + duration, data=train)
fit_interact <- lm(win ~ wards * position + duration, data=train)

pred_basic <- predict(fit_basic, newdata=test)
pred_interact <- predict(fit_interact, newdata=test)

mse_basic <- mean((test$win - pred_basic) ^ 2)
mse_interact <- mean((test$win - pred_interact) ^ 2)

print(c(mse_basic, mse_interact, var(test$win)))
```

It's safe to conclude that linear interactions between ward and positions do not provide a meaningful difference.

## 1.2:
Time to add more predictors

```{r}
df <- read.csv("data/d2_250_big.csv")
df$win <- df$win == "True"
```

```{r}
set.seed(67)
nrow <- nrow(df)
index <- sample(1:nrow, size = nrow * 0.7)
train <- df[index, ]
test <- df[-index, ]
```

```{r}
fit_big <- lm(win ~ ., data=train)
pred_big <- predict(fit_big, newdata=test)
mse_big <- mean((test$win - pred_big)^2)
mse_big
summary(fit_big)
```

```{r}
fit_reduced <- lm(win ~ ., data = train)
pred_reduced <- predict(fit_reduced, newdata=test)
mse_reduced <- mean((test$win - pred_reduced)^2)
mse_reduced
var(test$win)
```

My conclusion here is that pure playstyle predictors (pings, wards) are overall poor predictors of win. Performance-based predictors like turret takedowns or cctime are good predictors but are obviously correlated with the win through confounding factors like the gold they provide and the fact that already winning teams are going to have an easier time taking turrets.

I'm not sure exactly how to use this model to analyze a specific game yet either.

As a result, I think that I want to try these things in order:
1) Using the model to provide feedback on a specific game.
2) Fitting non-linear models to see if they can make sense of things better
3) Trying to focus on performance-based metrics to provide feedback (IE: "your CS is bad" or "you die too much")


## 1.3:

```{r}
df <- read.csv("data/d2_250_performance.csv")
df$win <- df$win == "True"
#df <- df[df$lane != "NONE"]
```

```{r}
set.seed(67)
nrow <- nrow(df)
index <- sample(1:nrow, size = nrow * 0.7)
train <- df[index, ]
test <- df[-index, ]
```

```{r}
colnames(df)
```

```{r}
fit_big <- lm(win ~ goldEarned * lane, data=train)
pred_big <- predict(fit_big, newdata=test)
mse_big <- mean((test$win - pred_big)^2)
mse_big
summary(fit_big)
```

## 1.4:
Let's fit a random forest model

```{r}
library(tidymodels)
library(tibble)
```

```{r}
colnames(df)
```

```{r}
df <- read.csv("data/d2_250_performance.csv")
df <- df[df$lane != "NONE", ]
df$position = NULL
df$win <- factor(df$win)
X <- model.matrix(win ~ . - 1, data=df)
X <- as.data.frame(X)
y <- df$win
```

```{r}
set.seed(67)
nrow <- nrow(df)
index <- sample(1:nrow, size = nrow * 0.7)
X_train <- X[index, ]
y_train <- y[index]
X_test <- X[-index, ]
y_test <- y[-index]
```

```{r}
rf <- rand_forest(
  mtry= 5,
  trees = 500,
  min_n = 5,
) %>%
  set_mode("classification") %>%
  set_engine("ranger", importance = "impurity") %>%
  fit_xy(X_train, y_train)
```

```{r}
pred <- predict(rf, new_data=X_test, type="class")

eval_df <- tibble(
  win = y_test,
  .pred_class = pred$.pred_class
)

accuracy(eval_df, truth = win, estimate = .pred_class)$.estimate
```
## 1.4.5:
Let's use SHAP to evaluate gameplay based on our random forest model.

```{r}
library(fastshap)
```

Okay let's take a random match:
```{r}
example_index <- 20
example <- X_test[example_index, ]
print(y_test[example_index])
print(paste("Deaths: ", example$deaths))
```
Looks like this game was a loss. The player also had 7 deaths (which is a lot and deaths are obviously not good!) So perhaps this contributed to the outcome.

```{r}
predict_fn <- function(object, newdata){
    predict(object, new_data = newdata, type = "prob")$.pred_True
}
```

```{r}
set.seed(67)

testexplain <- fastshap::explain(rf, X=X_test, pred_wrapper=predict_fn, newdata=example, nsim=1000,adjust=TRUE)
testexplain
```
This is a bunch of explanation of what contributed to this particular game. So we can see that deaths reduced the predicted win chance by 0.02 (the adjust setting makes this an actual probability). Turrets lost, however, accounted for an overwhelming majority of the problem.

## 1.5:
Let's fit a `glm` model using logistic regression.




